{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOPkZ7pAH1hD"
      },
      "source": [
        "# Objective: Forest vs Building Classification Using ANN\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "gLvsD4opN8N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wrqhxNtH1hM"
      },
      "source": [
        "Steps:\n",
        "1. Importing (or installing) Tenosrflow, Keras and other packages on your system\n",
        "2. Loading your data from disk\n",
        "3. Creating your training and testing splits\n",
        "4. Data Preprocessing\n",
        "5. Defining your tensorflow ANN model architecture\n",
        "6. Compiling your tensorflow ANN model\n",
        "7. Training your model on your training data\n",
        "8. Evaluating your model on your test data\n",
        "9. Generate Plots for accuracy and validation loss\n",
        "10. Saving The train model\n",
        "11. Making predictions using your trained tensorflow model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd-17cKpH1hO"
      },
      "source": [
        "### Step 1: Importing all the packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wc07XIN8H1hP"
      },
      "outputs": [],
      "source": [
        "# Import libraries and packages\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix , accuracy_score, classification_report\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from imutils import paths\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "import cv2\n",
        "import os\n",
        "import time   # time1 = time.time(); print('Time taken: {:.1f} seconds'.format(time.time() - time1))\n",
        "import warnings\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import itertools\n",
        "\n",
        "%matplotlib inline\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "SEED = 42   # set random seed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4ykOP9PH1hR"
      },
      "source": [
        "### Step 2: Loading your data from disk for training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "bjjzgZesXfBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1XcUZ0TF6nW"
      },
      "outputs": [],
      "source": [
        "# change working DIR\n",
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/Deep Learning/ANN\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!dir"
      ],
      "metadata": {
        "id": "xoxXRBT7Y2d0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZVekxG5F6nW"
      },
      "outputs": [],
      "source": [
        "# Extract dataset.rar file\n",
        "!pip install patool\n",
        "import patoolib\n",
        "patoolib.extract_archive(\"dataset.rar\")\n",
        "patoolib.extract_archive(\"test_examples.rar\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqf7ooW0F6nX"
      },
      "outputs": [],
      "source": [
        "!dir"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(paths.list_images('dataset'))[-11:]"
      ],
      "metadata": {
        "id": "5Sh1v0EScUDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'dataset/Forest/943.jpg'.split(\"/\")[-2]"
      ],
      "metadata": {
        "id": "i4bB7KIgdw3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6fWvIhpH1hS"
      },
      "outputs": [],
      "source": [
        "# initialize the data and labels\n",
        "print(\"[INFO] loading images...\")\n",
        "data = [] # X\n",
        "labels = [] # Y\n",
        "classes = [\"Forest\", \"Buildings\"]\n",
        "\n",
        "# grab the image paths and randomly shuffle them\n",
        "imagePaths = sorted(list(paths.list_images('dataset')))   # data folder with 2 categorical folders\n",
        "random.seed(SEED)\n",
        "random.shuffle(imagePaths)\n",
        "\n",
        "# progress bar\n",
        "with tqdm(total=len(imagePaths)) as pbar:\n",
        "\n",
        "    # loop over the input images\n",
        "    for imagePath in imagePaths:\n",
        "        # load the image, resize the image to be 32x32 pixels (ignoring aspect ratio),\n",
        "        # flatten the 32x32x3=3072 pixel image into a list, and store the image in the data list\n",
        "        image = cv2.imread(imagePath)\n",
        "        image = cv2.resize(image, (32, 32)).flatten()\n",
        "        data.append(image)\n",
        "\n",
        "        # extract the class label from the image path and update the labels list\n",
        "        label = imagePath.split(os.path.sep)[-2]\n",
        "\n",
        "        label = 1 if label == \"Buildings\" else 0\n",
        "        labels.append(label)\n",
        "\n",
        "        # update the progressbar\n",
        "        pbar.update(1)\n",
        "\n",
        "# scale the raw pixel intensities to the range [0, 1]\n",
        "data = np.array(data, dtype=\"float\") / 255.0\n",
        "labels = np.array(labels)\n",
        "\n",
        "print(\"[INFO] Done...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVPfN9uLH1hV"
      },
      "outputs": [],
      "source": [
        "print(\"Total Images: \", len(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4t61EG6aH1hW"
      },
      "outputs": [],
      "source": [
        "# sample data for first image\n",
        "print(\"sample image: {}\".format(data[0]))\n",
        "print(\"no of features/pixels values: {}\".format(len(data[0]))) # 32x32x3=3072\n",
        "print(\"label: {}\".format(classes[labels[0]]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(data)\n",
        "df[\"Label\"] = labels\n",
        "df.head()"
      ],
      "metadata": {
        "id": "WJoA3M2sBRAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1om_oqHH1hX"
      },
      "source": [
        "### Step 3: Creating your training and testing splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JeZsctwH1hY"
      },
      "outputs": [],
      "source": [
        "# partition the data into 80% training and 20% validation\n",
        "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.2, random_state=SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LS-WxAlMH1hZ"
      },
      "outputs": [],
      "source": [
        "trainX.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HgWZSTVH1hZ"
      },
      "outputs": [],
      "source": [
        "trainY.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "383Ph1VjH1ha"
      },
      "outputs": [],
      "source": [
        "testX.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uUniwLqH1ha"
      },
      "outputs": [],
      "source": [
        "testY.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZUchGj5H1hb"
      },
      "outputs": [],
      "source": [
        "trainX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOQ2xVXWH1he"
      },
      "outputs": [],
      "source": [
        "trainY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGXcQ96NH1he"
      },
      "outputs": [],
      "source": [
        "type(trainY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2xgBvbyH1hf"
      },
      "outputs": [],
      "source": [
        "testY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ju5PeM9IH1hf"
      },
      "source": [
        "### Step 4: Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOVY7aNvH1hg"
      },
      "outputs": [],
      "source": [
        "# convert the labels from integers/categories to vectors\n",
        "\n",
        "trainY = to_categorical(trainY, num_classes=2)   # fit_transform = find all unique class labels + transform into one-hot encoded labels\n",
        "testY = to_categorical(testY, num_classes=2)     # transform = perform the one-hot encoding (unique class labels already found)\n",
        "\n",
        "# [0,1] Buildings\n",
        "# [1,0] Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSIXmTiAH1hg"
      },
      "outputs": [],
      "source": [
        "# testY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6a8Kt4OAH1hg"
      },
      "outputs": [],
      "source": [
        "trainY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqa_ionjH1hh"
      },
      "outputs": [],
      "source": [
        "sample_image = (trainX[75] * 255).astype(\"int\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CvUQMryH1hh"
      },
      "outputs": [],
      "source": [
        "plt.imshow(sample_image.reshape(32,32,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6M9W37ywH1hh"
      },
      "outputs": [],
      "source": [
        "trainY[75]   # [0,1] means buildings [1,0] means forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWaKJJQAH1hi"
      },
      "source": [
        "### Step 5:  Define the architecture for ANN MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sn6MdCz1H1hi"
      },
      "outputs": [],
      "source": [
        "# define the 3072-1024-512-1 architecture using Keras\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# input layer 3072 as there are 32x32x3=3072 pixels in a flattened input image\n",
        "# first hidden layer has 1024 nodes\n",
        "model.add(Dense(units = 1024, input_shape=(3072,), kernel_initializer = 'uniform', activation=\"relu\"))\n",
        "\n",
        "# # dropout for second layer\n",
        "# model.add(Dropout(0.4))\n",
        "\n",
        "# second hidden layer has 512 nodes\n",
        "model.add(Dense(units=512, kernel_initializer='uniform', activation=\"relu\"))\n",
        "\n",
        " # output layer with number of possible class labels\n",
        "model.add(Dense(units=2,kernel_initializer='uniform', activation=\"softmax\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4TsihI_H1hi"
      },
      "source": [
        "## Compile Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH_Z8uwLH1hj"
      },
      "source": [
        "### Step 6:  Compiling your tensorflow ANN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RfbU0kQH1hj"
      },
      "outputs": [],
      "source": [
        "# initialize our initial learning rate and # of epochs to train for\n",
        "INIT_LR = 0.01\n",
        "EPOCHS = 50\n",
        "\n",
        "# compile the model using SGD as our optimizer and categorical cross-entropy loss\n",
        "# (you'll want to use binary_crossentropy for 2-class classification)\n",
        "\n",
        "\n",
        "print(\"[INFO] compiling network network...\")\n",
        "opt = SGD(learning_rate=INIT_LR)   # Stochastic Gradient Descent (SGD) optimizer\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YUcdkObH1hj"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2VDhQ7tH1hk"
      },
      "source": [
        "### Step 7: Training your model on your training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLiN6EKYH1hk"
      },
      "source": [
        "#### Fit (ie, Train) model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JM-4rOxH1hk",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# train the neural network on training data set\n",
        "# batch_size (32) controls the size of each group of data to pass through the network.\n",
        "H = model.fit(trainX,\n",
        "              trainY,\n",
        "            #   validation_data=(testX, testY),\n",
        "              validation_split=0.1,\n",
        "              epochs=EPOCHS,\n",
        "              batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LA1TugnpH1hl"
      },
      "source": [
        "### Step 8: Evaluating your model on your test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "207w4HO_H1hl"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVjWkFsoH1hm"
      },
      "outputs": [],
      "source": [
        "# evaluate the network\n",
        "print(\"[INFO] evaluating network...\")\n",
        "pred_prob = model.predict(testX, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-gWgwFmH1ho"
      },
      "source": [
        "### Convert testY and y_pred into 1's and 0 for classification report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7DmFQsjH1ho"
      },
      "outputs": [],
      "source": [
        "# Note: buildings -> 1 and forest -> 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DBLMzCzH1ho"
      },
      "outputs": [],
      "source": [
        "test_y = [ np.argmax(i)  for i in testY]\n",
        "pred_y = [ np.argmax(i)  for i in pred_prob]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2Az8TLkH1hp"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_metrix(y_true, y_pred,classes,\n",
        "                         normalize=False,\n",
        "                         title='Confusion Matrix',\n",
        "                         cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    Objective\n",
        "    ----------\n",
        "    plot confussion matrix, classification report and accuracy score\n",
        "\n",
        "    parameters\n",
        "    ----------\n",
        "    y_true : array-like of shape (n_samples,)\n",
        "        Ground truth (correct) target values.\n",
        "\n",
        "    y_pred : array-like of shape (n_samples,)\n",
        "        Estimated targets as returned by a classifier.\n",
        "\n",
        "    classes : list\n",
        "        List of labels to index the matrix\n",
        "\n",
        "    title : title for matrix\n",
        "    cmap : colormap for matrix\n",
        "\n",
        "    returns\n",
        "    ----------\n",
        "   all accruacy matrix\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    cm = confusion_matrix(y_true,y_pred)\n",
        "\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized Confusion Matrix\")\n",
        "    else:\n",
        "        print(\"Confusion Matrix, Without Normalisation\")\n",
        "\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest',cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks,classes,rotation=35)\n",
        "    plt.yticks(tick_marks,classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() /2.\n",
        "\n",
        "    for i , j in itertools.product(range(cm.shape[0]), range(cm.shape[0])):\n",
        "        plt.text(j, i, format(cm[i,j], fmt),\n",
        "                 horizontalalignment='center',\n",
        "                 color='white' if cm[i, j] > thresh else 'black')\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    # plt.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    print(\"-----------------------------------------------------\")\n",
        "    print('Classification report')\n",
        "    print(classification_report(y_true,y_pred))\n",
        "\n",
        "    print(\"-----------------------------------------------------\")\n",
        "    acc= accuracy_score(y_true,y_pred)\n",
        "    print(\"Accuracy of the model: \", acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyVfoLGNH1hp",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "plot_confusion_metrix(test_y, pred_y,classes=[\"Forest: 0\",\"Buildings: 1\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJqB_fgnH1hq"
      },
      "source": [
        "### Step 9: Generate Plots for acc and val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZLMjjR9H1hq"
      },
      "outputs": [],
      "source": [
        "# plot the training and validation loss\n",
        "N = np.arange(0, EPOCHS)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure(figsize = [10,8])\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.title(\"ANN: Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch #\", weight=\"bold\")\n",
        "plt.ylabel(\"Loss\", weight=\"bold\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5De4qVQ4H1hq"
      },
      "outputs": [],
      "source": [
        "# plot the training and validation accuracy\n",
        "N = np.arange(0, EPOCHS)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure(figsize = [10,8])\n",
        "plt.plot(N, H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(N, H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"ANN: Training and Validation Accuracy\")\n",
        "plt.xlabel(\"Epoch #\", weight=\"bold\")\n",
        "plt.ylabel(\"Accuracy\", weight=\"bold\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shtColjWH1hr"
      },
      "outputs": [],
      "source": [
        "# accuracy = 88%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdGFIrlQH1hr"
      },
      "source": [
        "### Step 10: Saving the train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywefocd8H1hs"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBZCa0oJH1hs"
      },
      "outputs": [],
      "source": [
        "# save the model and label binarizer to disk\n",
        "print(\"[INFO] serializing network and label binarizer...\")\n",
        "model.save('model_ANN.h5')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6qjHck8H1ht"
      },
      "source": [
        "### Step 11: Making predictions using your trained tensorflow model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXLgFC1yH1ht"
      },
      "outputs": [],
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.models import load_model\n",
        "import pickle\n",
        "import cv2\n",
        "import imutils\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_twfMl7H1ht"
      },
      "outputs": [],
      "source": [
        "# load the model\n",
        "print(\"[INFO] loading network and...\")\n",
        "model2 = load_model(\"model_ANN.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrCsgnI5H1hu"
      },
      "outputs": [],
      "source": [
        "def display_img(img):\n",
        "    fig = plt.figure(figsize=(12,10))\n",
        "    # plt.grid(b=None)\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dt9L1h68H1hu",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# load the input image and resize it to the target spatial dimensions\n",
        "width = 32\n",
        "height = 32\n",
        "\n",
        "# grab the image paths and randomly shuffle them\n",
        "testImagePaths = sorted(list(paths.list_images('test_examples')))   # test data folder with random images\n",
        "\n",
        "\n",
        "# progress bar\n",
        "with tqdm(total=len(testImagePaths)) as pbar:\n",
        "\n",
        "    for imagePath in testImagePaths:\n",
        "        image = cv2.imread(imagePath)\n",
        "        output = image.copy()\n",
        "        image = cv2.resize(image, (width, height))\n",
        "\n",
        "        # scale the pixel values to [0, 1]\n",
        "        image = image.astype(\"float\") / 255.0\n",
        "\n",
        "        # for a simple fully-connected network, flatten the image\n",
        "        image = image.flatten()\n",
        "        image = image.reshape((1, image.shape[0]))\n",
        "\n",
        "\n",
        "        # make a prediction on the image\n",
        "        preds = model2.predict(image)\n",
        "\n",
        "        # find the class label index with the largest corresponding probability\n",
        "        i = preds.argmax(axis=1)[0]\n",
        "        label = classes[i]\n",
        "\n",
        "        label = \"{}: {:.2f}%\".format(label, preds[0][i] * 100)\n",
        "\n",
        "\n",
        "        output = imutils.resize(output, width=400)\n",
        "        cv2.putText(output, label, (10, 25),  cv2.FONT_HERSHEY_SIMPLEX,\n",
        "            0.7, (0, 255, 0), 2)\n",
        "\n",
        "        # convert img to rgb format and display in noteboo\n",
        "        img = cv2.cvtColor(output, cv2.COLOR_BGR2RGB)\n",
        "        display_img(img)\n",
        "\n",
        "#         print(\"############################\")\n",
        "#         print(\"image: {}\".format(os.path.split(imagePath)[-1]))\n",
        "#         print(\"predicted label: {}\".format(label))\n",
        "#         print(\"Confidence: {}\".format(preds[0][i]))\n",
        "\n",
        "        pbar.update(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAyLD_WRH1hv"
      },
      "source": [
        "## Deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ML2hzEuwH1hw"
      },
      "outputs": [],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kArJAiWyH1hw"
      },
      "outputs": [],
      "source": [
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdQAzpp_H1hw"
      },
      "outputs": [],
      "source": [
        "def predict_image(image):\n",
        "\n",
        "    image = cv2.resize(image, (32, 32))\n",
        "\n",
        "    # scale the pixel values to [0, 1]\n",
        "    image = image.astype(\"float\") / 255.0\n",
        "\n",
        "    # for a simple fully-connected network, flatten the image\n",
        "    image = image.flatten()\n",
        "    image = image.reshape((1, image.shape[0]))\n",
        "\n",
        "    # make a prediction on the image\n",
        "    preds = model.predict(image).flatten()\n",
        "    result = dict()\n",
        "    result[\"Forest\"] = round(float(list(preds)[0]), 3)\n",
        "    result[\"Buildings\"] = round(float(list(preds)[1]), 3)\n",
        "    print(result)\n",
        "\n",
        "    return result\n",
        "\n",
        "im = gr.inputs.Image(shape=(32,32))\n",
        "label = gr.outputs.Label(num_top_classes=2)\n",
        "\n",
        "gr.Interface(fn=predict_image, inputs=im, outputs=label, capture_session=True, title=\"ANN Demo\").launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FIWwwwkH1hx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf_gpu",
      "language": "python",
      "name": "tf_gpu"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}